---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Shallow Neural Network

```{python}
from src.data import SNPDataModule
from src.models import ShallowNN

datamodule = SNPDataModule()
datamodule.setup()

x, y = next(iter(datamodule.train_dataloader()))  # train one batch
```

A shallow neural network consists of a single hidden layer with an activation function, in this case SiLU (Sigmoid Linear Unit), also known as Swish.

```{python}
net = ShallowNN(datamodule.num_features, datamodule.num_classes)
net
```

The forward pass of this network returns a vector of non-normalized predictions (logits) genered by the log softmax function.

```{python}
logits = net(x)
logits.shape
```

Behind the scenes, the logits are used to determine the likeliest class and compared it to the target, thus deriving the model's accuracy. The negative log-likelihood is used to determine the loss.

```{python}
loss, acc = net.calculate_loss((x, y))
loss.item(), acc
```

# Model Training

PyTorch Lightning does away with the tedious job of writing a training loop.

To train with this library, one must:
1. Set up a logger object that will save hyperparameters and metrics at each epoch/step.
2. Define callbacks (such as early stopping).
3. Set up a trainer object to configure the number of epochs and type of accelerator used during training, among other things.

```{python}
import pytorch_lightning as pl
from src.models import CSVLogger

# 1. Set up a logger object
#    This logger will store the metrics in a CSV file stored under "models/{name}/{version}"
logger = CSVLogger(name="shallow_nn", version="test", metrics=["loss", "acc"])

# 2. Set up callbacks
early_stopping = pl.callbacks.EarlyStopping(monitor="val_loss")

# 3. Set up trainer and train
trainer = pl.Trainer(
    logger,
    accelerator="cpu",
    max_epochs=5,
    limit_train_batches=1,  # just to test, prevents training all batches
    limit_val_batches=1,
    callbacks=[early_stopping],
)
trainer.fit(net, datamodule=datamodule)
```

To make things even easier and more compact, it has been condensed into the `train_model` function.

```{python}
from src.models import train_model

# warning: do not run! it will run at max 400 epochs and take some time
train_model(model=net, datamodule=datamodule, logger=logger)
```

# Checking results

The CSV logger saves results to a CSV located under `models/{name}/{version}`.

```{python}
logger.log_dir
```

This file can be loaded using Pandas, which the `_load_metrics` method does.

```{python}
from src.visualization.metrics import _load_metrics

_load_metrics(logger.log_dir)
```

Finallt, the metrics can be plotted using the `plot_metrics` function, which uses the information in the logger object to load the metrics and hyperparameters and display a plot for each tracked metric (loss and accuracy in this case).

```{python}
from src.visualization import plot_metrics

plot_metrics(logger, figsize=(6, 3))
```

```{python}

```
